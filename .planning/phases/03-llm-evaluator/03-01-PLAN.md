# Phase 3: LLM Evaluator - Execution Plan

## Overview

Integrate OpenAI Enterprise API for nuanced quality assessments (60% of scoring). The LLM evaluates subjective criteria that require understanding context, quality of communication, and troubleshooting effectiveness.

## Dependencies

- Phase 2 (Rules Engine) complete
- OpenAI Enterprise API access configured
- Existing models: ServiceNowTicket, RuleResult, TemplateType

## Research Completed

- OpenAI Python SDK v1.x uses `openai.OpenAI()` client
- Structured outputs via `response_format={"type": "json_object"}`
- GPT-4o recommended for accuracy/cost balance
- Rate limits: 10,000 TPM for Enterprise, implement exponential backoff

## Tasks

### Task 1: OpenAI Client Setup

**File**: `src/tqrs/llm/client.py`

Create OpenAI client wrapper with:
- Environment variable configuration (OPENAI_API_KEY, OPENAI_API_BASE for Enterprise)
- Retry logic with exponential backoff (3 retries, 1s/2s/4s delays)
- Timeout handling (30s default)
- Token usage tracking
- Error classification (rate limit vs API error vs validation error)

```python
class OpenAIClient:
    def __init__(self, api_key: str | None = None, base_url: str | None = None):
        # Load from env if not provided

    async def complete(self, messages: list[dict], response_schema: dict) -> dict:
        # With retry logic and structured output
```

**Acceptance**: Client connects, handles errors gracefully, returns structured JSON.

---

### Task 2: Evaluation Response Models

**File**: `src/tqrs/llm/schemas.py`

Define Pydantic models for LLM responses:

```python
class CriterionEvaluation(BaseModel):
    criterion_id: str
    score: int
    max_score: int
    evidence: list[str]  # Quotes from ticket
    reasoning: str
    strengths: list[str]
    improvements: list[str]
    coaching: str

class DescriptionEvaluation(CriterionEvaluation):
    """20 pts - Accurate description of issue/request"""
    completeness_score: int  # 0-10
    clarity_score: int  # 0-10

class TroubleshootingEvaluation(CriterionEvaluation):
    """20 pts - Quality of troubleshooting steps"""
    steps_documented: bool
    logical_progression: bool
    appropriate_actions: bool

class ResolutionEvaluation(CriterionEvaluation):
    """15 pts - Resolution notes quality"""
    outcome_clear: bool
    steps_documented: bool
    confirmation_obtained: bool

class CustomerServiceEvaluation(CriterionEvaluation):
    """20 pts - Soft skills and customer interaction"""
    professional_tone: bool
    empathy_shown: bool
    clear_communication: bool

class SpellingGrammarEvaluation(CriterionEvaluation):
    """2 pts - Writing quality"""
    errors_found: list[str]
    severity: str  # minor, moderate, significant

class LLMEvaluationResponse(BaseModel):
    """Complete LLM evaluation for a ticket"""
    ticket_number: str
    template_type: str
    evaluations: list[CriterionEvaluation]
    overall_assessment: str
    total_llm_score: int
    max_llm_score: int
```

**Acceptance**: Models validate LLM responses, handle missing fields gracefully.

---

### Task 3: Prompt Templates

**File**: `src/tqrs/llm/prompts.py`

Create structured prompts for each evaluation criterion:

#### 3a. Description Evaluation Prompt (20 pts)
- Evaluate: Issue clarity, context provided, user impact stated
- Scoring guide: 18-20 (excellent), 14-17 (good), 10-13 (adequate), <10 (poor)

#### 3b. Troubleshooting Evaluation Prompt (20 pts)
- Evaluate: Steps documented, logical sequence, appropriate actions
- Special handling: Reset/restart tickets need clear action documentation

#### 3c. Resolution Notes Prompt (15 pts)
- Evaluate: Clear outcome, steps taken, user confirmation
- Score higher for explicit confirmation obtained

#### 3d. Customer Service Prompt (20 pts)
- Evaluate: Professional tone, empathy, clear communication
- Check for: Greeting, explanation quality, closing

#### 3e. Spelling/Grammar Prompt (2 pts)
- Count significant errors (typos, grammar issues)
- 0 errors = 2 pts, 1-2 errors = 1 pt, 3+ errors = 0 pts

Each prompt includes:
- System role with scoring rubric
- Ticket data (description, work_notes, close_notes)
- JSON schema for structured response
- Examples of good/poor scores

**Acceptance**: Prompts produce consistent, parseable JSON responses.

---

### Task 4: LLM Evaluator Orchestrator

**File**: `src/tqrs/llm/evaluator.py`

```python
class LLMEvaluator:
    def __init__(self, client: OpenAIClient):
        self.client = client
        self.prompts = PromptTemplates()

    async def evaluate_ticket(
        self,
        ticket: ServiceNowTicket,
        template: TemplateType
    ) -> list[RuleResult]:
        """Run all LLM evaluations for a ticket"""

    async def evaluate_description(self, ticket) -> RuleResult:
        """20 pts - Accurate description"""

    async def evaluate_troubleshooting(self, ticket) -> RuleResult:
        """20 pts - Troubleshooting quality"""

    async def evaluate_resolution(self, ticket) -> RuleResult:
        """15 pts - Resolution notes"""

    async def evaluate_customer_service(self, ticket) -> RuleResult:
        """20 pts - Customer service quality"""

    async def evaluate_spelling_grammar(self, ticket) -> RuleResult:
        """2 pts - Writing quality"""
```

Features:
- Parallel evaluation where possible (asyncio.gather)
- Template-specific evaluation selection
- Converts LLM responses to RuleResult format
- Aggregates coaching recommendations

**Acceptance**: Returns list[RuleResult] compatible with rules engine output.

---

### Task 5: Batch Processing Support

**File**: `src/tqrs/llm/batch.py`

```python
class BatchLLMEvaluator:
    def __init__(self, evaluator: LLMEvaluator, concurrency: int = 5):
        self.evaluator = evaluator
        self.semaphore = asyncio.Semaphore(concurrency)

    async def evaluate_batch(
        self,
        tickets: list[ServiceNowTicket],
        template: TemplateType,
        progress_callback: Callable | None = None
    ) -> list[TicketEvaluation]:
        """Evaluate multiple tickets with rate limiting"""
```

Features:
- Concurrent evaluation with semaphore (default 5 parallel)
- Progress tracking for UI integration
- Error isolation (one ticket failure doesn't stop batch)
- Token usage aggregation

**Acceptance**: Process 50 tickets in <5 minutes with progress updates.

---

### Task 6: Integration Tests

**File**: `tests/test_llm_evaluator.py`

Tests with mocked OpenAI responses:

1. **Client tests**: Retry logic, timeout handling, error classification
2. **Schema tests**: Response validation, partial response handling
3. **Prompt tests**: Correct prompt construction per template
4. **Evaluator tests**: Full evaluation flow with mock responses
5. **Batch tests**: Concurrency, progress tracking, error isolation

**Real API tests** (optional, requires API key):
- Mark with `@pytest.mark.integration`
- Test against prototype samples
- Validate scores within expected ranges

**Acceptance**: 90%+ coverage on LLM module, all mocked tests pass.

---

### Task 7: Configuration and Documentation

**Files**:
- `src/tqrs/config.py` (update)
- `README.md` (update)

Add configuration:
```python
class LLMConfig(BaseModel):
    model: str = "gpt-4o"
    temperature: float = 0.1  # Low for consistency
    max_tokens: int = 2000
    timeout: int = 30
    max_retries: int = 3
    batch_concurrency: int = 5
```

Update README with:
- Environment variable setup (OPENAI_API_KEY, OPENAI_API_BASE)
- LLM evaluator usage examples
- Expected token costs per ticket (~1500-2000 tokens)

**Acceptance**: Configuration loads from environment, documentation complete.

---

## Validation Criteria

| Criterion | Target |
|-----------|--------|
| All prototype samples score 70/70 | Required |
| LLM scores within 5% of expected | Required |
| Batch processing <5 min for 50 tickets | Required |
| Test coverage >85% for LLM module | Required |
| Graceful handling of API failures | Required |

## Files Created/Modified

### New Files
- `src/tqrs/llm/__init__.py`
- `src/tqrs/llm/client.py`
- `src/tqrs/llm/schemas.py`
- `src/tqrs/llm/prompts.py`
- `src/tqrs/llm/evaluator.py`
- `src/tqrs/llm/batch.py`
- `tests/test_llm_evaluator.py`

### Modified Files
- `src/tqrs/config.py`
- `pyproject.toml` (add openai dependency)
- `README.md`

## Execution Order

1. Task 1 (Client) - Foundation for all LLM operations
2. Task 2 (Schemas) - Response models needed for prompts
3. Task 3 (Prompts) - Core evaluation logic
4. Task 4 (Evaluator) - Orchestrates individual evaluations
5. Task 5 (Batch) - Production-ready batch processing
6. Task 6 (Tests) - Validate all components
7. Task 7 (Config) - Finalize configuration and docs

## Notes

- Use `gpt-4o` for best accuracy/cost balance (~$0.005/ticket)
- Temperature 0.1 for consistent scoring
- All prompts include scoring rubric to ensure consistency
- Structured output ensures parseable responses
- Retry logic handles transient API failures
