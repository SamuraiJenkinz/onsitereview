# Phase 4: Scoring Engine - Execution Plan

## Overview

Combine rules engine and LLM evaluation results into final 70-point scores. Apply deductions, handle auto-fail conditions, assign performance bands, and determine pass/fail status.

## Dependencies

- Phase 2 (Rules Engine): `RulesEvaluator`, `RuleResult`
- Phase 3 (LLM Evaluator): `LLMEvaluator`, `BatchLLMEvaluator`
- Existing models: `EvaluationResult`, `CriterionScore`, `PerformanceBand`, `BatchEvaluationSummary`

## Template Scoring Breakdown

### Incident Logging (70 pts)
| Criterion | Max Pts | Source |
|-----------|---------|--------|
| Critical Process | PASS/-35/FAIL | Rules |
| Validation | PASS/-15/FAIL | Rules |
| Category | 10 | Rules |
| Subcategory | 10 | Rules |
| Service | 10 | Rules |
| Configuration Item | 10 | Rules |
| Short Description | 8 | Rules |
| Accurate Description | 20 | LLM |
| Spelling/Grammar | 2 | LLM |
| **Base Total** | **70** | |

### Incident Handling (70 pts)
| Criterion | Max Pts | Source |
|-----------|---------|--------|
| Critical Process | PASS/-35/FAIL | Rules |
| Validation | PASS/-15/FAIL | Rules |
| Priority | 5 | Rules/LLM |
| Troubleshooting | 20 | LLM |
| Interaction vs Incident | 5 | Rules |
| Routing/Resolving | 20 | LLM |
| Resolution Code | 5 | Rules |
| Resolution Notes | 15 | LLM |
| **Base Total** | **70** | |

### Customer Service (70 pts)
| Criterion | Max Pts | Source |
|-----------|---------|--------|
| Critical Process | PASS/-35/FAIL | Rules |
| Validation | PASS/-15/FAIL | Rules |
| Greeting | 5 | LLM |
| Offer Work Around | 10 | LLM |
| Necessary Troubleshooting | 10 | LLM |
| Self-Resolve Training | 10 | LLM |
| Resolution Follow-through | 10 | LLM |
| Closing Message | 5 | LLM |
| General Customer Service | 20 | LLM |
| **Base Total** | **70** | |

## Tasks

### Task 1: Scoring Calculator

**File**: `src/tqrs/scoring/calculator.py`

Core scoring logic that combines rule and LLM results:

```python
class ScoringCalculator:
    def __init__(self):
        self.template_max_scores = {
            TemplateType.INCIDENT_LOGGING: 70,
            TemplateType.INCIDENT_HANDLING: 70,
            TemplateType.CUSTOMER_SERVICE: 70,
        }

    def calculate_score(
        self,
        rule_results: list[RuleResult],
        llm_results: list[RuleResult],
        template: TemplateType,
    ) -> ScoringResult:
        """Calculate final score from combined results."""

    def apply_deductions(
        self,
        base_score: int,
        rule_results: list[RuleResult],
    ) -> tuple[int, int, int]:
        """Apply validation and critical process deductions."""

    def check_auto_fail(
        self,
        rule_results: list[RuleResult],
    ) -> tuple[bool, str | None]:
        """Check for auto-fail conditions."""
```

**Scoring Rules**:
1. Sum all positive numeric scores from rules and LLM
2. Apply validation deduction (-15 if documented but not properly)
3. Apply critical process deduction (-35 for non-password failures)
4. Cap minimum score at 0 (never negative)
5. Auto-fail if validation FAIL or password critical process FAIL

**Acceptance**: Calculator produces correct scores for prototype samples.

---

### Task 2: Template Score Maps

**File**: `src/tqrs/scoring/templates.py`

Define criterion mappings for each template:

```python
@dataclass
class TemplateCriterion:
    criterion_id: str
    criterion_name: str
    max_points: int
    source: Literal["rules", "llm", "both"]
    required: bool = True

INCIDENT_LOGGING_CRITERIA: list[TemplateCriterion] = [
    TemplateCriterion("critical_process_followed", "Critical Process", 0, "rules"),
    TemplateCriterion("validation_performed", "Validation", 0, "rules"),
    TemplateCriterion("correct_category", "Category", 10, "rules"),
    TemplateCriterion("correct_subcategory", "Subcategory", 10, "rules"),
    TemplateCriterion("correct_service", "Service", 10, "rules"),
    TemplateCriterion("correct_ci", "Configuration Item", 10, "rules"),
    TemplateCriterion("short_description_format", "Short Description", 8, "rules"),
    TemplateCriterion("accurate_description", "Description", 20, "llm"),
    TemplateCriterion("spelling_grammar", "Spelling/Grammar", 2, "llm"),
]

# Similar for INCIDENT_HANDLING_CRITERIA and CUSTOMER_SERVICE_CRITERIA
```

**Acceptance**: Template maps match scoring_rubrics.json exactly.

---

### Task 3: Ticket Evaluator (Orchestrator)

**File**: `src/tqrs/scoring/evaluator.py`

Main orchestrator that runs full evaluation pipeline:

```python
class TicketEvaluator:
    def __init__(
        self,
        rules_evaluator: RulesEvaluator,
        llm_evaluator: LLMEvaluator,
        calculator: ScoringCalculator,
    ):
        ...

    def evaluate_ticket(
        self,
        ticket: ServiceNowTicket,
        template: TemplateType,
    ) -> EvaluationResult:
        """Run complete evaluation and return final result."""
        # 1. Run rules evaluation
        # 2. Run LLM evaluation
        # 3. Combine results
        # 4. Calculate score with deductions
        # 5. Check auto-fail
        # 6. Assign performance band
        # 7. Build EvaluationResult
```

**Acceptance**: Returns fully populated EvaluationResult with all fields.

---

### Task 4: Batch Evaluator

**File**: `src/tqrs/scoring/batch.py`

Batch processing with progress tracking:

```python
class BatchTicketEvaluator:
    def __init__(
        self,
        evaluator: TicketEvaluator,
        concurrency: int = 5,
    ):
        ...

    def evaluate_batch(
        self,
        tickets: list[ServiceNowTicket],
        template: TemplateType,
        progress_callback: Callable | None = None,
    ) -> BatchEvaluationResult:
        """Evaluate multiple tickets and generate summary."""

    def generate_summary(
        self,
        results: list[EvaluationResult],
    ) -> BatchEvaluationSummary:
        """Generate batch statistics."""
```

Features:
- Concurrent evaluation with semaphore
- Progress tracking compatible with Streamlit
- Error isolation per ticket
- Batch summary with statistics

**Acceptance**: Process 50 tickets with progress updates and summary.

---

### Task 5: Result Formatting

**File**: `src/tqrs/scoring/formatter.py`

Format results for display and export:

```python
class ResultFormatter:
    def to_criterion_scores(
        self,
        rule_results: list[RuleResult],
        llm_results: list[RuleResult],
        template: TemplateType,
    ) -> list[CriterionScore]:
        """Convert RuleResults to CriterionScores."""

    def collect_strengths(
        self,
        results: list[RuleResult],
    ) -> list[str]:
        """Extract strength statements from high-scoring criteria."""

    def collect_improvements(
        self,
        results: list[RuleResult],
    ) -> list[str]:
        """Extract improvement areas from low-scoring criteria."""

    def get_coaching_recommendations(
        self,
        results: list[RuleResult],
    ) -> list[str]:
        """Collect all coaching recommendations."""
```

**Acceptance**: Formatted results match EvaluationResult schema.

---

### Task 6: Integration Tests

**File**: `tests/test_scoring.py`

Comprehensive tests:

1. **Calculator tests**:
   - Score calculation for each template
   - Deduction application (validation, critical process)
   - Auto-fail handling
   - Minimum score capping

2. **Template tests**:
   - Criterion mapping completeness
   - Max score validation (sum = 70)

3. **Evaluator tests**:
   - Full pipeline with mocked LLM
   - EvaluationResult field population

4. **Batch tests**:
   - Concurrent evaluation
   - Summary statistics
   - Error handling

5. **Prototype sample tests**:
   - INC8924218: Expected 70/70
   - INC8924339: Expected 70/70
   - INC8923651: Expected 70/70

**Acceptance**: All tests pass, coverage >85% for scoring module.

---

### Task 7: Module Integration

**Files**:
- `src/tqrs/scoring/__init__.py`
- Update `src/tqrs/__init__.py`

Export public API:

```python
from tqrs.scoring import (
    ScoringCalculator,
    TicketEvaluator,
    BatchTicketEvaluator,
    ResultFormatter,
)
```

Add convenience function:

```python
def evaluate_tickets(
    tickets: list[ServiceNowTicket],
    template: TemplateType,
    api_key: str,
    progress_callback: Callable | None = None,
) -> BatchEvaluationResult:
    """High-level API for batch evaluation."""
```

**Acceptance**: Clean public API, imports work correctly.

---

## Validation Criteria

| Criterion | Target |
|-----------|--------|
| All prototype samples score 70/70 | Required |
| Performance bands assigned correctly | Required |
| Deductions applied correctly | Required |
| Auto-fail triggers correctly | Required |
| Batch processing with progress | Required |
| Test coverage >85% | Required |

## Files Created/Modified

### New Files
- `src/tqrs/scoring/__init__.py`
- `src/tqrs/scoring/calculator.py`
- `src/tqrs/scoring/templates.py`
- `src/tqrs/scoring/evaluator.py`
- `src/tqrs/scoring/batch.py`
- `src/tqrs/scoring/formatter.py`
- `tests/test_scoring.py`

### Modified Files
- `src/tqrs/__init__.py` (add scoring exports)

## Execution Order

1. Task 2 (Templates) - Define criterion mappings first
2. Task 1 (Calculator) - Core scoring logic
3. Task 5 (Formatter) - Result formatting utilities
4. Task 3 (Evaluator) - Orchestrate full pipeline
5. Task 4 (Batch) - Batch processing
6. Task 6 (Tests) - Validate all components
7. Task 7 (Integration) - Finalize module exports

## Notes

- All templates sum to 70 points maximum
- Deductions apply after base score calculation
- Minimum score is 0 (never negative)
- Auto-fail overrides score calculation
- Performance band determined from final percentage
- Pass threshold is 90% (63/70 = 90%)
