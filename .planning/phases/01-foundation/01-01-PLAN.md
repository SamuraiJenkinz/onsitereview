# Plan 01-01: Foundation Setup

## Objective
Establish the complete project foundation: scaffolding, ServiceNow JSON parser, Pydantic data models, and configuration management. Validates against prototype samples.

## Prerequisites
- Python 3.12+ installed
- OpenAI API key available (for Phase 3, but config now)
- Access to prototype_samples.json and scoring_rubrics.json

## Tasks

### Task 1: Project Scaffolding
**Create Python package structure with modern tooling**

Create:
- `pyproject.toml` with project metadata and dependencies
- `src/tqrs/` package structure
- `tests/` directory structure
- `.env.example` for environment variables
- `README.md` for project (basic)

Dependencies to include:
- `pydantic>=2.0` - Data validation and models
- `openai>=1.0` - OpenAI API client
- `streamlit>=1.30` - Web UI
- `jinja2>=3.0` - HTML templating
- `plotly>=5.0` - Visualizations
- `python-dotenv>=1.0` - Environment config
- `pytest>=8.0` - Testing
- `ruff>=0.1` - Linting

Package structure:
```
src/
  tqrs/
    __init__.py
    parser/
      __init__.py
      servicenow.py      # JSON parsing
    models/
      __init__.py
      ticket.py          # ServiceNow ticket model
      evaluation.py      # Evaluation result models
      rubric.py          # Scoring rubric models
    config/
      __init__.py
      settings.py        # Pydantic settings
tests/
  __init__.py
  test_parser.py
  test_models.py
  conftest.py            # Pytest fixtures
```

**Verification**:
- [ ] `python -c "import tqrs"` succeeds
- [ ] `pip install -e .` completes without errors
- [ ] `ruff check src/` passes
- [ ] Directory structure matches specification

---

### Task 2: Configuration Management
**Implement Pydantic Settings for environment-based config**

Create `src/tqrs/config/settings.py`:
- `Settings` class using `pydantic_settings`
- Environment variable loading from `.env`
- Configuration for:
  - `OPENAI_API_KEY` (required, loaded from env)
  - `OPENAI_MODEL` (default: "gpt-4o")
  - `BATCH_SIZE` (default: 50)
  - `LOG_LEVEL` (default: "INFO")
  - `TEMP_DIR` (default: system temp)

Create `.env.example`:
```
OPENAI_API_KEY=your-key-here
OPENAI_MODEL=gpt-4o
BATCH_SIZE=50
LOG_LEVEL=INFO
```

**Verification**:
- [ ] Settings loads from `.env` file
- [ ] Missing OPENAI_API_KEY raises clear error
- [ ] Default values work when optional vars missing
- [ ] `from tqrs.config import settings` works

---

### Task 3: ServiceNow Ticket Model
**Define Pydantic models for parsed ticket data**

Create `src/tqrs/models/ticket.py`:

```python
class ServiceNowTicket(BaseModel):
    """Parsed ServiceNow incident ticket with relevant fields."""

    # Identifiers
    number: str                    # INC8924218
    sys_id: str                    # Unique identifier

    # Timestamps
    opened_at: datetime
    resolved_at: datetime | None
    closed_at: datetime | None

    # People
    caller_id: str                 # Sys_id reference
    opened_by: str                 # Sys_id reference
    assigned_to: str               # Sys_id reference
    resolved_by: str | None
    closed_by: str | None

    # Classification
    category: str
    subcategory: str
    contact_type: str              # phone, chat, email, self-service
    priority: str                  # 1-5
    impact: str                    # 1-3
    urgency: str                   # 1-3

    # Content (evaluation targets)
    short_description: str
    description: str
    work_notes: str
    close_notes: str
    close_code: str

    # Status
    state: str                     # 1-7 (7=closed)
    incident_state: str

    # Business context
    company: str                   # Sys_id reference
    location: str                  # Sys_id reference
    assignment_group: str          # Sys_id reference
    business_service: str | None
    cmdb_ci: str | None

    # Line of Business flags
    u_marsh: bool = False
    u_mercer: bool = False
    u_guy_carpenter: bool = False
    u_oliver_wyman_group: bool = False
    u_mmc_corporate: bool = False

    # Metadata
    reassignment_count: int = 0
    reopen_count: int = 0

    # Computed fields (populated by parser)
    line_of_business: str | None = None
    resolution_time_minutes: int | None = None
```

Key considerations:
- Handle empty strings as None where appropriate
- Parse boolean strings ("true"/"false") to bool
- Parse datetime strings to datetime objects
- Compute `line_of_business` from u_* flags or short_description

**Verification**:
- [ ] Model validates against all 3 prototype samples
- [ ] Invalid data raises clear ValidationError
- [ ] Boolean flag strings parse correctly
- [ ] Datetime parsing handles ServiceNow format

---

### Task 4: Evaluation Result Models
**Define models for evaluation outputs**

Create `src/tqrs/models/evaluation.py`:

```python
class CriterionScore(BaseModel):
    """Individual criterion evaluation result."""
    criterion_id: str              # e.g., "short_description_format"
    criterion_name: str            # Human readable
    max_points: int
    points_awarded: int
    evidence: str                  # Quote or reason
    reasoning: str                 # Why this score
    coaching: str | None           # Improvement suggestion

class TemplateType(str, Enum):
    INCIDENT_LOGGING = "incident_logging"
    INCIDENT_HANDLING = "incident_handling"
    CUSTOMER_SERVICE = "customer_service"

class PerformanceBand(str, Enum):
    BLUE = "blue"       # >= 95%
    GREEN = "green"     # >= 90%
    YELLOW = "yellow"   # >= 75%
    RED = "red"         # >= 50%
    PURPLE = "purple"   # < 50%

class EvaluationResult(BaseModel):
    """Complete evaluation for one ticket."""
    ticket_number: str
    template: TemplateType

    # Scores
    total_score: int               # 0-70
    max_score: int = 70
    percentage: float              # 0-100
    band: PerformanceBand
    passed: bool                   # >= 90% (63/70)

    # Breakdown
    criterion_scores: list[CriterionScore]

    # Deductions
    validation_deduction: int = 0  # 0, -15, or FAIL
    critical_process_deduction: int = 0  # 0, -35, or FAIL
    auto_fail: bool = False
    auto_fail_reason: str | None = None

    # Summary
    strengths: list[str]
    improvements: list[str]

    # Metadata
    evaluated_at: datetime
    evaluation_time_seconds: float
```

**Verification**:
- [ ] Model correctly calculates percentage from score
- [ ] Band assignment matches spec thresholds
- [ ] Pass/fail logic at 63/70 threshold
- [ ] Serializes to JSON cleanly for reporting

---

### Task 5: Scoring Rubric Models
**Parse and model the scoring rubrics**

Create `src/tqrs/models/rubric.py`:

```python
class ScoringOption(BaseModel):
    """One scoring choice for a criterion."""
    label: str                     # "Excellent", "Good", etc.
    points: int
    description: str | None = None

class Criterion(BaseModel):
    """Single evaluation criterion."""
    id: str
    name: str
    max_points: int
    category: str                  # "documentation", "troubleshooting", etc.
    evaluation_type: str           # "rules" or "llm"
    options: list[ScoringOption]
    notes: str | None = None

class ScoringRubric(BaseModel):
    """Complete rubric for one template."""
    template: TemplateType
    total_points: int = 70
    criteria: list[Criterion]

    def get_criterion(self, criterion_id: str) -> Criterion | None:
        return next((c for c in self.criteria if c.id == criterion_id), None)
```

Create loader function to parse `scoring_rubrics.json`:
```python
def load_rubrics(path: Path) -> dict[TemplateType, ScoringRubric]:
    """Load all rubrics from JSON file."""
```

**Verification**:
- [ ] Parses scoring_rubrics.json without errors
- [ ] All 3 templates load with correct criteria counts
- [ ] Total points sum to 70 per template
- [ ] Can retrieve individual criteria by ID

---

### Task 6: ServiceNow JSON Parser
**Implement robust JSON parsing for ServiceNow exports**

Create `src/tqrs/parser/servicenow.py`:

```python
class ServiceNowParser:
    """Parse ServiceNow JSON exports into ticket models."""

    def parse_file(self, path: Path) -> list[ServiceNowTicket]:
        """Parse JSON file, return list of tickets."""

    def parse_json(self, data: dict) -> list[ServiceNowTicket]:
        """Parse JSON dict with 'records' key."""

    def _parse_ticket(self, raw: dict) -> ServiceNowTicket:
        """Parse single ticket record."""

    def _parse_bool(self, value: str) -> bool:
        """Convert 'true'/'false' strings to bool."""

    def _parse_datetime(self, value: str) -> datetime | None:
        """Parse 'YYYY-MM-DD HH:MM:SS' format."""

    def _extract_line_of_business(self, raw: dict) -> str | None:
        """Determine LoB from flags or short_description."""

    def _compute_resolution_time(
        self, opened: datetime, resolved: datetime | None
    ) -> int | None:
        """Calculate resolution time in minutes."""
```

Parsing considerations:
- Handle nested `{"records": [...]}` structure
- Skip records with `__status` != "success"
- Handle empty strings as None for optional fields
- Parse ServiceNow timestamps: "YYYY-MM-DD HH:MM:SS"
- Extract LoB from u_marsh, u_mercer, etc. flags or parse from short_description prefix

**Verification**:
- [ ] Parses prototype_samples.json successfully
- [ ] Returns exactly 3 tickets
- [ ] All ticket fields populated correctly
- [ ] LoB correctly identified for each sample
- [ ] Resolution time calculated accurately

---

### Task 7: Tests and Validation
**Create pytest tests validating against prototype samples**

Create `tests/conftest.py`:
```python
@pytest.fixture
def sample_tickets_path() -> Path:
    return Path(__file__).parent.parent / "prototype_samples.json"

@pytest.fixture
def sample_tickets(sample_tickets_path) -> list[ServiceNowTicket]:
    parser = ServiceNowParser()
    return parser.parse_file(sample_tickets_path)

@pytest.fixture
def rubrics_path() -> Path:
    return Path(__file__).parent.parent / "scoring_rubrics.json"
```

Create `tests/test_parser.py`:
- Test parsing prototype_samples.json
- Test each ticket's key fields
- Test datetime parsing
- Test boolean parsing
- Test LoB extraction

Create `tests/test_models.py`:
- Test ticket model validation
- Test evaluation result model
- Test rubric loading
- Test edge cases (empty strings, missing fields)

**Verification**:
- [ ] `pytest tests/` passes all tests
- [ ] Coverage for parser and models > 80%
- [ ] Each prototype sample validated
- [ ] Edge cases handled gracefully

---

## Execution Order

```
Task 1 (Scaffolding)
    ↓
Task 2 (Config) ←── can start after Task 1
    ↓
Task 3 (Ticket Model) ←── can start after Task 1
Task 4 (Eval Model) ←── can start after Task 3
Task 5 (Rubric Model) ←── can start after Task 3
    ↓
Task 6 (Parser) ←── needs Tasks 3, 5
    ↓
Task 7 (Tests) ←── needs all above
```

Parallel opportunities:
- Tasks 3, 4, 5 can proceed in parallel after Task 1
- Task 2 is independent after Task 1

## Success Criteria

Phase 1 is complete when:

1. **Structure**: `pip install -e .` works, `import tqrs` works
2. **Config**: Settings load from environment with sensible defaults
3. **Parser**: All 3 prototype samples parse without errors
4. **Models**: Pydantic validation catches invalid data with clear errors
5. **Tests**: pytest passes with >80% coverage on parser/models
6. **Quality**: `ruff check src/` passes with no errors

## Estimated Complexity

- **Tasks**: 7
- **New files**: ~15
- **Lines of code**: ~500-700
- **Risk**: Low (standard Python patterns)

## Notes

- No OpenAI calls in this phase - just structure and parsing
- All models designed to support JSON serialization for reporting
- Parser is intentionally lenient - validates structure, not business rules
- Rules engine (Phase 2) will use these models for evaluation logic
